\section{Standard approach}
\label{Standard}

We started with implementing a data streaming technique. Since our goal is to find heavy hitters, we chose Misra-Gries algorithm to save space and to process large amount of data fast.

\subsection{Pseudocode}
\label{MisraGries}
The idea of our version of Misra-Gries is to process data movie by movie. Each movie has a list of actors and while processing them we generate all triplets for each movie. Then we maintain a hash table which contains pairs of triplets and their respective count indicating relative number of occurences of the triplet. We add a triplet to the table if it does not appear there yet or we increment count of analyzed triplet. The result of the analysis is the triplet with the highest count. For more details we present pseudocode of the algorithm in paragraph below.
\\
Let m denote number of movies and A(i) denotes list of actors playing in a given movie i.
Let H be a hash table containing k pairs: triplet (t) and its count (\(count_t)\).
\begin{verbatim}
Algorithm 1: MisraGries()
1	FOREACH movie DO
2	  FOR i ← 0 to size of A(movie) DO
3	    FOR j ← i + 1 to size of A(movie) DO
4	      FOR k ← j + 1 to size of A(movie) DO
5	        IF {A(movie)[i],A(movie)[j],A(movie)[k]} ∈ H THEN
6	          count[t] += 1;
7	        ELSE
8	          INSERT({A(movie)[i],A(movie)[j],A(movie)[k]}, count);  
9	        END IF
10	       IF k < H.length THEN
11	       FOREACH t IN H DO
12	         count[t] -= 1;
13	          IF count[t] = 0 THEN
14	            H.REMOVE(t);
15	          END IF
16	        END FOREACH
17	      END IF      
18	    END FOR
19	  END FOR
20	 END FOR
21	END FOREACH
22	RETURN H.MAX(count);	  	                    	  
\end{verbatim}

The algorithm allows to save memory since not store all processed data is stored. The efficiency and its running time highly depend on the cache size. The bigger it is, the slower algorithm computes and the more memory space we use. On the other hand with bigger table, we can work with bigger sample, which would result in an approximation that is closer to the actual answer.
\\
The cache size was determined based on calculations and experiments. We estimated that with IMDB database we have nearly 2 billion triplets for all the movies. We ran multiple expreiments with different cache sizes, and the number we found most acceptable in terms of running times was 20 000 triplets. This represents 0.001\% of the whole set. While the sample is relatively small, the running time with any bigger cache size would have been unacceptable. More on the running times in section \ref{Experiments}
\\
\subsection{Analysis}
\label{AnalysisMisraGries}
Memory usage in Misra-Gries algorithm is very little, the space is needed only for the hash table. Complexity of the algorithm highly depends on the size of cache and the dataset to be processed. The bigger they are, the longer computation time is needed. The complexity of the algorithm is \(\sum\limits_{i=1}^m{a_i \choose 3}*k\) where k is size cache size. We can reduce it to \(\sum\limits_{i=1}^m{a_i^3}*k\).